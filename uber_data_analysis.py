# -*- coding: utf-8 -*-
"""Uber_Data_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GsBhQ3cuFuPSZZkPspSFY2CZTq_iFXv-

# Uber Data Analysis

In this project, we study the data of Uber which is present in tabular format in which we use different libraries like numpy, pandas and matplotlib and different machine learning algorithms.

We study different columns of the table and try to co-relate them with others and find a relation between those two.
"""

!pip install kaggle

from google.colab import files

uploaded = files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d zusmani/uberdrives

!unzip uberdrives.zip

"""## 2. Collecting the data
___________________________________
"""

import pandas as pd

uber = pd.read_csv(r"/content/My Uber Drives - 2016.csv")

"""## 3. Exploratory data analysis
__________________________________________________

Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.

It is a good practice to understand the data first and try to gather as many insights from it.

EDA is all about making sense of data in hand.

"""

uber.head()

uber.shape

uber.columns

# Overview of the dataset
print("\nOverview of the Dataset:")
print(uber.info())

# General Statistics
print("\nGeneral Statistics:")
print(uber.describe())

# Purpose Analysis
print("\nPurpose Analysis:")
print(uber['PURPOSE*'].value_counts())





# Plotting
import matplotlib.pyplot as plt
import seaborn as sns

# Bar plot for Purpose Analysis
plt.figure(figsize=(10, 5))
sns.countplot(y='PURPOSE*', data=uber, order=uber['PURPOSE*'].value_counts().index)
plt.title('Purpose Analysis')
plt.xlabel('Count')
plt.ylabel('Purpose')
plt.show()

# Location Analysis - Horizontal Bar Plot
# Calculate the number of unique start locations
start_locations = uber['START*'].nunique()

# Calculate the number of unique end locations
end_locations = uber['STOP*'].nunique()

# Location Analysis - Horizontal Bar Plot
locations_data = {'Location Type': ['Start Locations', 'End Locations'],
                  'Number of Locations': [start_locations, end_locations],
                  'Color': ['darkblue', 'red']}

locations_df = pd.DataFrame(locations_data)

plt.figure(figsize=(10, 6))
sns.barplot(x='Number of Locations', y='Location Type', data=locations_df, palette=locations_df['Color'])
plt.title('Start and End Locations Analysis')
plt.xlabel('Number of Locations')
plt.ylabel('Location Type')
plt.show()

# Pie chart for Category Analysis
plt.figure(figsize=(8, 8))
uber['CATEGORY*'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, colors=['aqua', 'red'])
plt.title('Category Distribution')
plt.ylabel('')
plt.show()

# Drop the 'Totals' row
uber = uber[uber['START_DATE*'] != 'Totals']

# Convert 'START_DATE*' and 'END_DATE*' to datetime objects
uber['START_DATE*'] = pd.to_datetime(uber['START_DATE*'])
uber['END_DATE*'] = pd.to_datetime(uber['END_DATE*'])

# Extract month and hour from 'START_DATE*'
uber['Month'] = uber['START_DATE*'].dt.month
uber['Hour'] = uber['START_DATE*'].dt.hour

# Monthly Analysis
plt.figure(figsize=(12, 6))
sns.countplot(x='Month', data=uber, color ='red')
plt.title('Trips Distribution Over Months')
plt.xlabel('Month')
plt.ylabel('Number of Trips')
plt.show()

# Hourly Analysis - Line Plot
plt.figure(figsize=(12, 6))
sns.lineplot(x='Hour', y='Count', data=uber.groupby('Hour').size().reset_index(name='Count'), marker='o', color='purple')
plt.title('Trips Distribution Over Hours')
plt.xlabel('Hour of the Day')
plt.ylabel('Number of Trips')
plt.grid(True)
plt.show()

""" Uber Data Analysis

1. **Purpose Analysis:**
   - Counts and lists trip purposes.
   - Visualizes purpose distribution with a bar plot.

2. **Location Analysis:**
   - Counts unique start and end locations.
   - Displays counts with a horizontal bar plot.

3. **Category Analysis:**
   - Shows the proportion of trip categories in a pie chart.

4. **Monthly Analysis:**
   - Illustrates trip distribution across months with a bar plot.

5. **Hourly Analysis:**
   - Plots a line graph to reveal how trip numbers change throughout the day.


"""

# Fill missing values for numerical columns with the mean
uber['MILES*'].fillna(uber['MILES*'].mean(), inplace=True)

# Fill missing values for categorical columns with the mode
uber['END_DATE*'].fillna(uber['END_DATE*'].mode()[0], inplace=True)
uber['CATEGORY*'].fillna(uber['CATEGORY*'].mode()[0], inplace=True)
uber['START*'].fillna(uber['START*'].mode()[0], inplace=True)
uber['STOP*'].fillna(uber['STOP*'].mode()[0], inplace=True)

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming you want to classify based on the 'MILES*' column
\
uber['Number of Trips'] = pd.cut(uber['MILES*'], bins=[0, 10, 20, float('inf')], labels=['Low', 'Medium', 'High'])

# Select features and target variable
X = uber[['START*', 'STOP*', 'PURPOSE*']]
y = uber['Number of Trips']

# Encode categorical variables
X_encoded = pd.get_dummies(X, columns=['START*', 'STOP*', 'PURPOSE*'], drop_first=True)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

# Initialize the Random Forest Classifier model
rf_classifier = RandomForestClassifier(random_state=42)

# Perform grid search for hyperparameter tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=3, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Get the best parameters from the grid search
best_params = grid_search.best_params_

# Initialize the Random Forest Classifier model with the best parameters
rf_classifier_best = RandomForestClassifier(**best_params, random_state=42)

# Train the model
rf_classifier_best.fit(X_train, y_train)

# Make predictions on the test set
y_pred_rf_classifier_best = rf_classifier_best.predict(X_test)

# Evaluate the model
classification_rep = classification_report(y_test, y_pred_rf_classifier_best)
accuracy = accuracy_score(y_test, y_pred_rf_classifier_best)

print("Best Parameters:", best_params)
print("Accuracy:", accuracy)

print("Classification Report:\n", classification_rep)

# Calculate confusion matrix
cm = confusion_matrix(y_test, y_pred_rf_classifier_best)

# Plot confusion matrix as a heatmap with custom labels
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted Low', 'Predicted Medium', 'Predicted High'],
            yticklabels=['Actual Low', 'Actual Medium', 'Actual High'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

"""# **Observations:**

1. **Accuracy and Precision:**
   - The model achieved an overall accuracy of 74.57%, with high precision for 'Low' trips (78%), but lower precision for 'Medium' (62%) and 'High' (0%) categories.

2. **Recall:**
   - While recall for 'Low' trips is high (93%), it is lower for 'Medium' (44%) and 'High' (0%) trips, indicating the model's struggle to capture instances in these categories.

3. **F1-Score:**
   - The weighted average F1-score is 0.71, suggesting a reasonable balance between precision and recall.
"""